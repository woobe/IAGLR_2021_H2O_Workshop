---
title: "IAGLR 2021 H2O Workshop"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    number_sections: yes
    fig_height: 10
    fig_width: 14
    highlight: kate
    toc_depth: 3
    css: style.css
    
---


```{r knitr_init, echo=FALSE, cache=FALSE}
library(knitr)
library(rmdformats)
library(DT)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=FALSE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

# Agenda

- Welcomes and introductions – Timothy Maguire (UMich CIGLR) and Jo-fai Chow (H2O.ai) (5 mins)
- Great Lakes research questions driven by data – Timothy Maguire (15 mins)
- Introduction to the H2O.ai software – Jo-fai Chow (5 mins)
- Live-code example of data manipulation and machine learning analysis –  Jo-Fai Chow (20 mins)
- Results in context – Timothy Maguire (10 mins)
- Q&A (5 mins)


# Introduction to H2O

TBA


# Software and Code

## Code

- `setup.R`: install packages required
- `tutorial.Rmd`: the main RMarkdown file with code 
- `tutorial.html`: this webpage
- **GitHub Repo** https://github.com/woobe/IAGLR_2021_H2O_Workshop

## R Packages

- Check out `setup.R`
- For this tutorial:
    - `h2o` for automatic and explainable machine learning
- For RMarkdown
    - `knitr` for rendering this RMarkdown
    - `rmdformats` for `readthedown` RMarkdown template
    - `DT` for nice tables


# H2O Basics

```{r, message=FALSE}
# Let's go
library(h2o) # for H2O Machine Learning
```

## Start a local H2O Cluster (JVM)

```{r}
h2o.init()
```

```{r}
h2o.no_progress() # disable progress bar for RMarkdown
h2o.removeAll()   # Optional: remove anything from previous session 
```


```{r}
# Enter your lucky seed here ...
n_seed <- 12345
```



# Data - Dom Lake Huron Abund

```{r}
lake_data <- h2o.importFile('https://raw.githubusercontent.com/woobe/IAGLR_2021_H2O_Workshop/main/data/Dom_Lake_Huron_Abund.csv')
```

```{r}
datatable(as.data.frame(head(lake_data)), rownames = FALSE, options = list(pageLength = 10, scrollX = TRUE, round)) %>%
  formatRound(columns = -1, digits = 4)
```



```{r}
h2o.describe(lake_data)
```

```{r}
h2o.hist(lake_data$DPOL, breaks = 100)
```





```{r}
h2o.hist(lake_data$DBUG, breaks = 100)
```

## Define Target and Features

```{r}
target_DPOL <- "DPOL"
target_DBUG <- "DBUG"

# Remove targets, C1, and Dreisseniidae (which is DPOL + DBUG)
features <- setdiff(colnames(lake_data), c(target_DPOL, target_DBUG, "C1", "Dreisseniidae"))

print(features)
```

<center>
![ml_overview](img/ml_overview.png)
</center>


## Split Data into Train/Test

```{r}
h_split <- h2o.splitFrame(lake_data, ratios = 0.75, seed = n_seed)
h_train <- h_split[[1]] # 80% for modelling
h_test <- h_split[[2]] # 20% for evaluation
```

```{r}
dim(h_train)
dim(h_test)
```

## Cross-Validation

<center>
![CV](img/cross_validation.png)
</center>


# Worked Example - Target "DPOL"

## Baseline Models

- `h2o.glm()`: H2O Generalized Linear Model
- `h2o.randomForest()`: H2O Random Forest Model
- `h2o.gbm()`: H2O Gradient Boosting Model
- `h2o.deeplearning()`: H2O Deep Neural Network Model 
- `h2o.xgboost()`: H2O wrapper for eXtreme Gradient Boosting Model (XGBoost) from DMLC

Let's start with GBM

```{r}
model_gbm_DPOL <- h2o.gbm(x = features,               # All 13 features
                              y = target_DPOL,            # medv (median value of owner-occupied homes in $1000s)
                              training_frame = h_train,   # H2O dataframe with training data
                              nfolds = 5,                 # Using 5-fold CV
                              seed = n_seed)              # Your lucky seed
```

```{r}
# Cross-Validation
model_gbm_DPOL@model$cross_validation_metrics
```


```{r}
# Evaluate performance on test
h2o.performance(model_gbm_DPOL, newdata = h_test)
```

Let's use RMSE

<center>
![RMSE](img/rmse.jpg)
</center>


Build Other Baseline Models (GLM, DRF, GBM, DNN) - TRY IT YOURSELF!

```{r eval=FALSE}
# Try other H2O models
# model_glm <- h2o.glm(x = features, y = target, ...)
# model_gbm <- h2o.gbm(x = features, y = target, ...)
# model_drf <- h2o.randomForest(x = features, y = target, ...)
# model_dnn <- h2o.deeplearning(x = features, y = target, ...)
# model_xgb <- h2o.xgboost(x = features, y = target, ...)
```


## Manual Tuning

### Check out the hyper-parameters for each algo

```{r, eval=FALSE}
?h2o.glm 
?h2o.randomForest
?h2o.gbm
?h2o.deeplearning
?h2o.xgboost
```

### Train a xgboost model with manual settings

```{r}
model_gbm_DPOL_m <- h2o.gbm(x = features, 
                                y = target_DPOL, 
                                training_frame = h_train,
                                nfolds = 5,
                                seed = n_seed,
                                # Manual Settings based on experience
                                learn_rate = 0.1,       # use a lower rate (more conservative)
                                ntrees = 120,           # use more trees (due to lower learn_rate)
                                sample_rate = 0.7,      # use random n% of samples for each tree  
                                col_sample_rate = 0.7)  # use random n% of features for each tree
```

### Comparison (RMSE: Lower = Better)


```{r}
# Create a table to compare RMSE from different models
d_eval <- data.frame(model = c("GBM: Gradient Boosting Model (Baseline)",
                               "GBM: Gradient Boosting Model (Manual Settings)"),
                     stringsAsFactors = FALSE)
d_eval$RMSE_cv <- NA
d_eval$RMSE_test <- NA

# Store RMSE values
d_eval[1, ]$RMSE_cv <- model_gbm_DPOL@model$cross_validation_metrics@metrics$RMSE
d_eval[2, ]$RMSE_cv <- model_gbm_DPOL_m@model$cross_validation_metrics@metrics$RMSE
d_eval[1, ]$RMSE_test <- h2o.rmse(h2o.performance(model_gbm_DPOL, newdata = h_test))
d_eval[2, ]$RMSE_test <- h2o.rmse(h2o.performance(model_gbm_DPOL_m, newdata = h_test))

# Show Comparison (RMSE: Lower = Better)
datatable(d_eval, rownames = FALSE, options = list(pageLength = 10, scrollX = TRUE, round)) %>%
  formatRound(columns = -1, digits = 4)
```


## H2O AutoML

```{r}
# Run AutoML (try n different models)
# Check out all options using ?h2o.automl
automl_DPOL = h2o.automl(x = features,
                         y = target_DPOL,
                         training_frame = h_train,
                         nfolds = 5,                        # 5-fold Cross-Validation
                         max_models = 30,                   # Max number of models
                         stopping_metric = "RMSE",          # Metric to optimize
                         exclude_algos = c("deeplearning", "xgboost"), # exclude some alogs for a quick demo
                         seed = n_seed)
```

### Leaderboard

```{r}
datatable(as.data.frame(automl_DPOL@leaderboard), 
          rownames = FALSE, options = list(pageLength = 10, scrollX = TRUE, round)) %>%
  formatRound(columns = -1, digits = 4)
```

### Best Model (Leader)

```{r}
automl_DPOL@leader
```

### Comparison (RMSE: Lower = Better)

```{r}
d_eval_tmp <- data.frame(model = "Best Model from H2O AutoML",
                         RMSE_cv = automl_DPOL@leader@model$cross_validation_metrics@metrics$RMSE,
                         RMSE_test = h2o.rmse(h2o.performance(automl_DPOL@leader, newdata = h_test)))
d_eval <- rbind(d_eval, d_eval_tmp)

datatable(d_eval, rownames = FALSE, options = list(pageLength = 10, scrollX = TRUE, round)) %>%
  formatRound(columns = -1, digits = 4)
```



## Make Predictions

```{r}
yhat_test <- h2o.predict(automl_DPOL@leader, newdata = h_test)
head(yhat_test, 5)
```


## Explainable AI - Target "DPOL"

### Global Explanations

```{r}
# Explain Best Model from AutoML
h2o.explain(automl_DPOL@leader, newdata = h_test)
```


### Local Explanations

```{r}
h2o.explain_row(automl_DPOL@leader, newdata = h_test, row_index = 1)
```

### Local Contributions (for Tree-based Models)

```{r}
predictions <- h2o.predict(automl_DPOL@leader, newdata = h_test)
as.data.frame(head(predictions, 5))
```

```{r}
contributions <- h2o.predict_contributions(automl_DPOL@leader, newdata = h_test)

datatable(as.data.frame(head(contributions)), rownames = FALSE, options = list(pageLength = 10, scrollX = TRUE, round)) %>%
  formatRound(columns = -1, digits = 4)
```



# Your Turn - Target "DBUG"

Get your hands dirty!


# Q & A



